{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "1. Data Preparation\n",
    "   - Load the data\n",
    "   - Split into features and target\n",
    "   - Create train/test split\n",
    "\n",
    "2. Define Evaluation Metrics\n",
    "   - Accuracy, Precision, Recall, F1-score for win prediction\n",
    "   - Mean Absolute Error, Mean Squared Error for score prediction\n",
    "\n",
    "3. Model Comparison (for win prediction)\n",
    "   - Logistic Regression\n",
    "   - Random Forest\n",
    "   - Gradient Boosting (e.g., XGBoost)\n",
    "   - Support Vector Machines\n",
    "\n",
    "4. Model Comparison (for score prediction)\n",
    "   - Linear Regression\n",
    "   - Decision Trees\n",
    "   - Random Forest\n",
    "   - Gradient Boosting\n",
    "\n",
    "5. Cross-Validation\n",
    "   - Implement k-fold cross-validation for each model\n",
    "\n",
    "6. Hyperparameter Tuning\n",
    "   - Use GridSearchCV or RandomizedSearchCV for best models\n",
    "\n",
    "7. Final Model Selection\n",
    "   - Choose the best model based on cross-validation results\n",
    "   - Evaluate on the test set\n",
    "\n",
    "8. Save Best Models\n",
    "   - Save the best models and their corresponding scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add the project root to the Python path\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.   Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_all_years = pd.read_parquet('../data/03_processed/preprocessed_all_years.parquet')\n",
    "df_2016_plus = pd.read_parquet('../data/03_processed/preprocessed_2016_plus.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target_column, drop_columns=None, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the data into train, validation, and test sets based on seasons.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame\n",
    "    target_column (str): The name of the target column\n",
    "    drop_columns (list): List of column names to drop from features. If None, use all columns except target and 'season'\n",
    "    test_size (float): Proportion of data to use for test set\n",
    "    val_size (float): Proportion of non-test data to use for validation set\n",
    "    random_state (int): Random state for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort the DataFrame by season to ensure chronological order\n",
    "    df = df.sort_values('season')\n",
    "    \n",
    "    # Define columns to drop\n",
    "    if drop_columns is None:\n",
    "        drop_columns = []\n",
    "    drop_columns = set(drop_columns + [target_column, 'season'])\n",
    "    \n",
    "    # Select feature columns (all columns except those in drop_columns)\n",
    "    feature_columns = [col for col in df.columns if col not in drop_columns]\n",
    "    \n",
    "    # Split features (X) and target (y)\n",
    "    X = df[feature_columns]\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Get unique seasons\n",
    "    seasons = df['season'].unique()\n",
    "    \n",
    "    # Calculate the number of seasons for test and validation\n",
    "    n_seasons = len(seasons)\n",
    "    n_test_seasons = max(1, int(n_seasons * test_size))\n",
    "    n_val_seasons = max(1, int((n_seasons - n_test_seasons) * val_size))\n",
    "    \n",
    "    # Split seasons into train, validation, and test\n",
    "    test_seasons = seasons[-n_test_seasons:]\n",
    "    val_seasons = seasons[-(n_test_seasons + n_val_seasons):-n_test_seasons]\n",
    "    train_seasons = seasons[:-(n_test_seasons + n_val_seasons)]\n",
    "    \n",
    "    # Create masks for each split\n",
    "    test_mask = df['season'].isin(test_seasons)\n",
    "    val_mask = df['season'].isin(val_seasons)\n",
    "    train_mask = df['season'].isin(train_seasons)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "    df_all_years,\n",
    "    target_column='win',\n",
    "    drop_columns=[\n",
    "        'season_type',\n",
    "        'team_id',\n",
    "        'opponent_id',\n",
    "        'team_conference',\n",
    "        'opponent_conference',\n",
    "        'start_date'\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.  Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'logistic_regression__C': 0.1, 'logistic_regression__penalty': 'l2', 'logistic_regression__solver': 'lbfgs', 'poly__degree': 1}\n",
      "\n",
      "Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72      1405\n",
      "           1       0.72      0.72      0.72      1405\n",
      "\n",
      "    accuracy                           0.72      2810\n",
      "   macro avg       0.72      0.72      0.72      2810\n",
      "weighted avg       0.72      0.72      0.72      2810\n",
      "\n",
      "Training Accuracy: 0.7276\n",
      "Validation Accuracy: 0.7185\n",
      "Test Accuracy: 0.6978\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "Feature 'is_home': 0.4589\n",
      "Feature 'conference_game': 0.3897\n",
      "Feature 'defense_total_ppa_weighted': 0.3354\n",
      "Feature 'defense_passing_plays.ppa_weighted': 0.2818\n",
      "Feature 'offense_passing_plays.ppa_last_3': 0.2706\n",
      "Feature 'defense_passing_plays.ppa_last_3': 0.2546\n",
      "Feature 'offense_total_ppa_weighted': 0.2511\n",
      "Feature 'offense_passing_plays.total_ppa_last_3': 0.2418\n",
      "Feature 'offense_ppa_last_10': 0.2417\n",
      "Feature 'offense_passing_plays.ppa_weighted': 0.2381\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_improved_logistic_regression_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Create a pipeline with more steps and options\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        ('logistic_regression', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'poly__degree': [1],\n",
    "        'logistic_regression__C': [0.01, 0.1, 1, 10],\n",
    "        'logistic_regression__penalty': ['l2'],\n",
    "        'logistic_regression__solver': ['lbfgs']\n",
    "    }\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = best_model.predict(X_train)\n",
    "    val_predictions = best_model.predict(X_val)\n",
    "    test_predictions = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importances = abs(best_model.named_steps['logistic_regression'].coef_[0])\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Print top 10 feature importances\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    for name, importance in sorted(zip(feature_names, feature_importances), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"Feature '{name}': {importance:.4f}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Usage\n",
    "improved_model = create_improved_logistic_regression_model(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68      1405\n",
      "           1       0.68      0.69      0.68      1405\n",
      "\n",
      "    accuracy                           0.68      2810\n",
      "   macro avg       0.68      0.68      0.68      2810\n",
      "weighted avg       0.68      0.68      0.68      2810\n",
      "\n",
      "Training Accuracy: 0.8824\n",
      "Validation Accuracy: 0.6801\n",
      "Test Accuracy: 0.6661\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "Feature 'is_home': 0.0374\n",
      "Feature 'yardsPerPass_weighted': 0.0216\n",
      "Feature 'kickingPoints_weighted': 0.0173\n",
      "Feature 'puntReturnYards_weighted': 0.0166\n",
      "Feature 'defense_second_level_yards_weighted': 0.0162\n",
      "Feature 'defense_standard_downs.success_rate_weighted': 0.0161\n",
      "Feature 'defense_success_rate_weighted': 0.0159\n",
      "Feature 'offense_success_rate_weighted': 0.0152\n",
      "Feature 'totalYards_weighted': 0.0146\n",
      "Feature 'rushingTDs_weighted': 0.0145\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def create_lightweight_random_forest(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Create a pipeline with reduced complexity\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Handle null values\n",
    "        ('scaler', StandardScaler()),  # Scale features\n",
    "        ('rf', RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42))  # Reduced parameters\n",
    "    ])\n",
    "    \n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = pipeline.predict(X_train)\n",
    "    val_predictions = pipeline.predict(X_val)\n",
    "    test_predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Feature importance (top 10)\n",
    "    feature_importance = pipeline.named_steps['rf'].feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    for name, importance in sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"Feature '{name}': {importance:.4f}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Usage\n",
    "lightweight_model = create_lightweight_random_forest(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colingaffney/repos/personal-projects/college_football_predictor/venv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [23:52:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72      1405\n",
      "           1       0.72      0.70      0.71      1405\n",
      "\n",
      "    accuracy                           0.71      2810\n",
      "   macro avg       0.72      0.71      0.71      2810\n",
      "weighted avg       0.72      0.71      0.71      2810\n",
      "\n",
      "Training Accuracy: 0.7537\n",
      "Validation Accuracy: 0.7149\n",
      "Test Accuracy: 0.7026\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "Feature 'win_rate_last_5': 0.1065\n",
      "Feature 'yardsPerPass_weighted': 0.0653\n",
      "Feature 'win_rate_last_10': 0.0588\n",
      "Feature 'kickingPoints_weighted': 0.0523\n",
      "Feature 'is_home': 0.0236\n",
      "Feature 'rushingTDs_weighted': 0.0235\n",
      "Feature 'defense_success_rate_weighted': 0.0192\n",
      "Feature 'offense_success_rate_weighted': 0.0183\n",
      "Feature 'offense_passing_plays.success_rate_weighted': 0.0164\n",
      "Feature 'defense_success_rate_last_3': 0.0130\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def create_xgboost_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Handle null values\n",
    "        ('scaler', StandardScaler()),  # Scale features\n",
    "        ('xgb', XGBClassifier(\n",
    "            n_estimators=100,  # Number of boosting rounds\n",
    "            max_depth=3,       # Maximum tree depth\n",
    "            learning_rate=0.1, # Learning rate\n",
    "            subsample=0.8,     # Subsample ratio of the training instances\n",
    "            colsample_bytree=0.8, # Subsample ratio of columns when constructing each tree\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,  # Avoid warning about label encoder\n",
    "            eval_metric='logloss'     # Evaluation metric\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = pipeline.predict(X_train)\n",
    "    val_predictions = pipeline.predict(X_val)\n",
    "    test_predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Feature importance (top 10)\n",
    "    feature_importance = pipeline.named_steps['xgb'].feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    for name, importance in sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"Feature '{name}': {importance:.4f}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Usage\n",
    "xgboost_model = create_xgboost_model(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to aggregate win_rate_last_5\n",
      "Warning: Failed to aggregate win_rate_last_10\n",
      "Warning: Failed to aggregate yardsPerPass_weighted\n",
      "Warning: Failed to aggregate kickingPoints_weighted\n",
      "Warning: Failed to aggregate rushingTDs_weighted\n",
      "Warning: Failed to aggregate defense_success_rate_weighted\n",
      "Warning: Failed to aggregate offense_success_rate_weighted\n",
      "Warning: Failed to aggregate offense_passing_plays.success_rate_weighted\n",
      "Warning: Failed to aggregate defense_success_rate_last_3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'team_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/personal-projects/college_football_predictor/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'team_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m home_team_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m251\u001b[39m  \u001b[38;5;66;03m# Replace with actual team ID\u001b[39;00m\n\u001b[1;32m     77\u001b[0m away_team_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m130\u001b[39m  \u001b[38;5;66;03m# Replace with actual team ID\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m winner, win_probability \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_winner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhome_team_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maway_team_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxgboost_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_all_years\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted winner: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwinner\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWin probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwin_probability\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 56\u001b[0m, in \u001b[0;36mpredict_winner\u001b[0;34m(home_id, away_id, xgboost_model, df_all_years)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03mPredict the winner of a game given home and away team IDs.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Generate features\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_features_from_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhome_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maway_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_all_years\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to predict\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.5\u001b[39m\n",
      "Cell \u001b[0;32mIn[33], line 30\u001b[0m, in \u001b[0;36mgenerate_features_from_ids\u001b[0;34m(home_id, away_id, df_all_years)\u001b[0m\n\u001b[1;32m     27\u001b[0m team_stats \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(team_stats)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Get stats for home and away teams\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m home_stats \u001b[38;5;241m=\u001b[39m team_stats[\u001b[43mteam_stats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mteam_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m home_id]\n\u001b[1;32m     31\u001b[0m away_stats \u001b[38;5;241m=\u001b[39m team_stats[team_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m away_id]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Check if we have stats for both teams\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/personal-projects/college_football_predictor/venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/repos/personal-projects/college_football_predictor/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'team_id'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_features_from_ids(home_id, away_id, df_all_years):\n",
    "    \"\"\"\n",
    "    Generate features for prediction based on team IDs.\n",
    "    \"\"\"\n",
    "    def safe_agg(group, column):\n",
    "        try:\n",
    "            return group[column].mean()\n",
    "        except:\n",
    "            return 0  # Default value if aggregation fails\n",
    "\n",
    "    features = ['win_rate_last_5', 'win_rate_last_10', 'yardsPerPass_weighted', \n",
    "                'kickingPoints_weighted', 'rushingTDs_weighted', \n",
    "                'defense_success_rate_weighted', 'offense_success_rate_weighted', \n",
    "                'offense_passing_plays.success_rate_weighted', 'defense_success_rate_last_3']\n",
    "    \n",
    "    team_stats = {}\n",
    "    for feature in features:\n",
    "        try:\n",
    "            team_stats[feature] = df_all_years.groupby('team_id').apply(lambda x: safe_agg(x, feature))\n",
    "        except:\n",
    "            print(f\"Warning: Failed to aggregate {feature}\")\n",
    "            team_stats[feature] = pd.Series(0, index=df_all_years['team_id'].unique())\n",
    "\n",
    "    team_stats = pd.DataFrame(team_stats).reset_index()\n",
    "\n",
    "    # Get stats for home and away teams\n",
    "    home_stats = team_stats[team_stats['team_id'] == home_id]\n",
    "    away_stats = team_stats[team_stats['team_id'] == away_id]\n",
    "\n",
    "    # Check if we have stats for both teams\n",
    "    if home_stats.empty or away_stats.empty:\n",
    "        print(f\"Warning: No data found for home team ID {home_id} or away team ID {away_id}\")\n",
    "        return None\n",
    "\n",
    "    home_stats = home_stats.iloc[0]\n",
    "    away_stats = away_stats.iloc[0]\n",
    "\n",
    "    # Create feature dictionary\n",
    "    feature_dict = {}\n",
    "    for feature in features:\n",
    "        feature_dict[f'home_{feature}'] = home_stats[feature]\n",
    "        feature_dict[f'away_{feature}'] = away_stats[feature]\n",
    "    \n",
    "    feature_dict['is_home'] = 1  # This is always 1 for the home team in this context\n",
    "\n",
    "    return pd.DataFrame([feature_dict])\n",
    "\n",
    "def predict_winner(home_id, away_id, xgboost_model, df_all_years):\n",
    "    \"\"\"\n",
    "    Predict the winner of a game given home and away team IDs.\n",
    "    \"\"\"\n",
    "    # Generate features\n",
    "    features = generate_features_from_ids(home_id, away_id, df_all_years)\n",
    "    \n",
    "    if features is None:\n",
    "        return \"Unable to predict\", 0.5\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = xgboost_model.predict(features)[0]\n",
    "    probability = xgboost_model.predict_proba(features)[0]\n",
    "    \n",
    "    # Interpret result\n",
    "    if prediction == 1:\n",
    "        winner = \"Home Team\"\n",
    "        win_probability = probability[1]\n",
    "    else:\n",
    "        winner = \"Away Team\"\n",
    "        win_probability = probability[0]\n",
    "    \n",
    "    return winner, win_probability\n",
    "\n",
    "# Example usage\n",
    "home_team_id = 251  # Replace with actual team ID\n",
    "away_team_id = 130  # Replace with actual team ID\n",
    "\n",
    "winner, win_probability = predict_winner(home_team_id, away_team_id, xgboost_model, df_all_years)\n",
    "print(f\"Predicted winner: {winner}\")\n",
    "print(f\"Win probability: {win_probability:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "college_football_predictor",
   "language": "python",
   "name": "college_football_predictor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
