{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "1. Data Preparation\n",
    "   - Load the data\n",
    "   - Split into features and target\n",
    "   - Create train/test split\n",
    "\n",
    "2. Define Evaluation Metrics\n",
    "   - Accuracy, Precision, Recall, F1-score for win prediction\n",
    "   - Mean Absolute Error, Mean Squared Error for score prediction\n",
    "\n",
    "3. Model Comparison (for win prediction)\n",
    "   - Logistic Regression\n",
    "   - Random Forest\n",
    "   - Gradient Boosting (e.g., XGBoost)\n",
    "   - Support Vector Machines\n",
    "\n",
    "4. Model Comparison (for score prediction)\n",
    "   - Linear Regression\n",
    "   - Decision Trees\n",
    "   - Random Forest\n",
    "   - Gradient Boosting\n",
    "\n",
    "5. Cross-Validation\n",
    "   - Implement k-fold cross-validation for each model\n",
    "\n",
    "6. Hyperparameter Tuning\n",
    "   - Use GridSearchCV or RandomizedSearchCV for best models\n",
    "\n",
    "7. Final Model Selection\n",
    "   - Choose the best model based on cross-validation results\n",
    "   - Evaluate on the test set\n",
    "\n",
    "8. Save Best Models\n",
    "   - Save the best models and their corresponding scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add the project root to the Python path\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.   Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_all_years = pd.read_parquet('../data/03_processed/preprocessed_all_years.parquet')\n",
    "df_2016_plus = pd.read_parquet('../data/03_processed/preprocessed_2016_plus.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target_column, drop_columns=None, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the data into train, validation, and test sets based on seasons.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame\n",
    "    target_column (str): The name of the target column\n",
    "    drop_columns (list): List of column names to drop from features. If None, use all columns except target and 'season'\n",
    "    test_size (float): Proportion of data to use for test set\n",
    "    val_size (float): Proportion of non-test data to use for validation set\n",
    "    random_state (int): Random state for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort the DataFrame by season to ensure chronological order\n",
    "    df = df.sort_values('season')\n",
    "    \n",
    "    # Define columns to drop\n",
    "    if drop_columns is None:\n",
    "        drop_columns = []\n",
    "    drop_columns = set(drop_columns + [target_column, 'season'])\n",
    "    \n",
    "    # Select feature columns (all columns except those in drop_columns)\n",
    "    feature_columns = [col for col in df.columns if col not in drop_columns]\n",
    "    \n",
    "    # Split features (X) and target (y)\n",
    "    X = df[feature_columns]\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Get unique seasons\n",
    "    seasons = df['season'].unique()\n",
    "    \n",
    "    # Calculate the number of seasons for test and validation\n",
    "    n_seasons = len(seasons)\n",
    "    n_test_seasons = max(1, int(n_seasons * test_size))\n",
    "    n_val_seasons = max(1, int((n_seasons - n_test_seasons) * val_size))\n",
    "    \n",
    "    # Split seasons into train, validation, and test\n",
    "    test_seasons = seasons[-n_test_seasons:]\n",
    "    val_seasons = seasons[-(n_test_seasons + n_val_seasons):-n_test_seasons]\n",
    "    train_seasons = seasons[:-(n_test_seasons + n_val_seasons)]\n",
    "    \n",
    "    # Create masks for each split\n",
    "    test_mask = df['season'].isin(test_seasons)\n",
    "    val_mask = df['season'].isin(val_seasons)\n",
    "    train_mask = df['season'].isin(train_seasons)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "    df_all_years,\n",
    "    target_column='win',\n",
    "    drop_columns=[\n",
    "        'season_type',\n",
    "        'team_id',\n",
    "        'opponent_id',\n",
    "        'team_conference',\n",
    "        'opponent_conference',\n",
    "        'start_date'\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.  Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'logistic_regression__C': 0.1, 'logistic_regression__penalty': 'l2', 'logistic_regression__solver': 'lbfgs', 'poly__degree': 1}\n",
      "\n",
      "Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72      1405\n",
      "           1       0.72      0.72      0.72      1405\n",
      "\n",
      "    accuracy                           0.72      2810\n",
      "   macro avg       0.72      0.72      0.72      2810\n",
      "weighted avg       0.72      0.72      0.72      2810\n",
      "\n",
      "Training Accuracy: 0.7276\n",
      "Validation Accuracy: 0.7185\n",
      "Test Accuracy: 0.6978\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "Feature 'is_home': 0.4589\n",
      "Feature 'conference_game': 0.3897\n",
      "Feature 'defense_total_ppa_weighted': 0.3354\n",
      "Feature 'defense_passing_plays.ppa_weighted': 0.2818\n",
      "Feature 'offense_passing_plays.ppa_last_3': 0.2706\n",
      "Feature 'defense_passing_plays.ppa_last_3': 0.2546\n",
      "Feature 'offense_total_ppa_weighted': 0.2511\n",
      "Feature 'offense_passing_plays.total_ppa_last_3': 0.2418\n",
      "Feature 'offense_ppa_last_10': 0.2417\n",
      "Feature 'offense_passing_plays.ppa_weighted': 0.2381\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_improved_logistic_regression_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Create a pipeline with more steps and options\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        ('logistic_regression', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'poly__degree': [1],\n",
    "        'logistic_regression__C': [0.01, 0.1, 1, 10],\n",
    "        'logistic_regression__penalty': ['l2'],\n",
    "        'logistic_regression__solver': ['lbfgs']\n",
    "    }\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = best_model.predict(X_train)\n",
    "    val_predictions = best_model.predict(X_val)\n",
    "    test_predictions = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importances = abs(best_model.named_steps['logistic_regression'].coef_[0])\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Print top 10 feature importances\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    for name, importance in sorted(zip(feature_names, feature_importances), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"Feature '{name}': {importance:.4f}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Usage\n",
    "improved_model = create_improved_logistic_regression_model(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68      1405\n",
      "           1       0.68      0.69      0.68      1405\n",
      "\n",
      "    accuracy                           0.68      2810\n",
      "   macro avg       0.68      0.68      0.68      2810\n",
      "weighted avg       0.68      0.68      0.68      2810\n",
      "\n",
      "Training Accuracy: 0.8824\n",
      "Validation Accuracy: 0.6801\n",
      "Test Accuracy: 0.6661\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "Feature 'is_home': 0.0374\n",
      "Feature 'yardsPerPass_weighted': 0.0216\n",
      "Feature 'kickingPoints_weighted': 0.0173\n",
      "Feature 'puntReturnYards_weighted': 0.0166\n",
      "Feature 'defense_second_level_yards_weighted': 0.0162\n",
      "Feature 'defense_standard_downs.success_rate_weighted': 0.0161\n",
      "Feature 'defense_success_rate_weighted': 0.0159\n",
      "Feature 'offense_success_rate_weighted': 0.0152\n",
      "Feature 'totalYards_weighted': 0.0146\n",
      "Feature 'rushingTDs_weighted': 0.0145\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def create_lightweight_random_forest(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Create a pipeline with reduced complexity\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Handle null values\n",
    "        ('scaler', StandardScaler()),  # Scale features\n",
    "        ('rf', RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42))  # Reduced parameters\n",
    "    ])\n",
    "    \n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = pipeline.predict(X_train)\n",
    "    val_predictions = pipeline.predict(X_val)\n",
    "    test_predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Feature importance (top 10)\n",
    "    feature_importance = pipeline.named_steps['rf'].feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    for name, importance in sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"Feature '{name}': {importance:.4f}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Usage\n",
    "lightweight_model = create_lightweight_random_forest(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colingaffney/repos/personal-projects/college_football_predictor/venv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [23:52:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72      1405\n",
      "           1       0.72      0.70      0.71      1405\n",
      "\n",
      "    accuracy                           0.71      2810\n",
      "   macro avg       0.72      0.71      0.71      2810\n",
      "weighted avg       0.72      0.71      0.71      2810\n",
      "\n",
      "Training Accuracy: 0.7537\n",
      "Validation Accuracy: 0.7149\n",
      "Test Accuracy: 0.7026\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "Feature 'win_rate_last_5': 0.1065\n",
      "Feature 'yardsPerPass_weighted': 0.0653\n",
      "Feature 'win_rate_last_10': 0.0588\n",
      "Feature 'kickingPoints_weighted': 0.0523\n",
      "Feature 'is_home': 0.0236\n",
      "Feature 'rushingTDs_weighted': 0.0235\n",
      "Feature 'defense_success_rate_weighted': 0.0192\n",
      "Feature 'offense_success_rate_weighted': 0.0183\n",
      "Feature 'offense_passing_plays.success_rate_weighted': 0.0164\n",
      "Feature 'defense_success_rate_last_3': 0.0130\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def create_xgboost_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Handle null values\n",
    "        ('scaler', StandardScaler()),  # Scale features\n",
    "        ('xgb', XGBClassifier(\n",
    "            n_estimators=100,  # Number of boosting rounds\n",
    "            max_depth=3,       # Maximum tree depth\n",
    "            learning_rate=0.1, # Learning rate\n",
    "            subsample=0.8,     # Subsample ratio of the training instances\n",
    "            colsample_bytree=0.8, # Subsample ratio of columns when constructing each tree\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,  # Avoid warning about label encoder\n",
    "            eval_metric='logloss'     # Evaluation metric\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = pipeline.predict(X_train)\n",
    "    val_predictions = pipeline.predict(X_val)\n",
    "    test_predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Feature importance (top 10)\n",
    "    feature_importance = pipeline.named_steps['xgb'].feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    for name, importance in sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"Feature '{name}': {importance:.4f}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Usage\n",
    "xgboost_model = create_xgboost_model(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tree must be Booster, XGBModel or dict instance",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_importance\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mplot_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxgboost_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/repos/personal-projects/college_football_predictor/venv/lib/python3.12/site-packages/xgboost/plotting.py:96\u001b[0m, in \u001b[0;36mplot_importance\u001b[0;34m(booster, ax, height, xlim, ylim, title, xlabel, ylabel, fmap, importance_type, max_num_features, grid, show_values, values_format, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     importance \u001b[38;5;241m=\u001b[39m booster\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree must be Booster, XGBModel or dict instance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m importance:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBooster.get_score() results in empty.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis maybe caused by having all trees as decision dumps.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: tree must be Booster, XGBModel or dict instance"
     ]
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_importance(xgboost_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "college_football_predictor",
   "language": "python",
   "name": "college_football_predictor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
