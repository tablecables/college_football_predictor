{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "1. Data Preparation\n",
    "   - Load the data\n",
    "   - Split into features and target\n",
    "   - Create train/test split\n",
    "\n",
    "2. Define Evaluation Metrics\n",
    "   - Accuracy, Precision, Recall, F1-score for win prediction\n",
    "   - Mean Absolute Error, Mean Squared Error for score prediction\n",
    "\n",
    "3. Model Comparison (for win prediction)\n",
    "   - Logistic Regression\n",
    "   - Random Forest\n",
    "   - Gradient Boosting (e.g., XGBoost)\n",
    "   - Support Vector Machines\n",
    "\n",
    "4. Model Comparison (for score prediction)\n",
    "   - Linear Regression\n",
    "   - Decision Trees\n",
    "   - Random Forest\n",
    "   - Gradient Boosting\n",
    "\n",
    "5. Cross-Validation\n",
    "   - Implement k-fold cross-validation for each model\n",
    "\n",
    "6. Hyperparameter Tuning\n",
    "   - Use GridSearchCV or RandomizedSearchCV for best models\n",
    "\n",
    "7. Final Model Selection\n",
    "   - Choose the best model based on cross-validation results\n",
    "   - Evaluate on the test set\n",
    "\n",
    "8. Save Best Models\n",
    "   - Save the best models and their corresponding scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add the project root to the Python path\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.   Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_db_path = '../data/04_features/features_teams.db'\n",
    "conn = sqlite3.connect(target_db_path)\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "df_all_years = pd.read_sql_query(\"SELECT * FROM features_teams\", conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_all_years = pd.read_parquet('../data/03_processed/preprocessed_all_years.parquet')\n",
    "df_2016_plus = pd.read_parquet('../data/03_processed/preprocessed_2016_plus.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target_column, drop_columns=None, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the data into train, validation, and test sets based on years.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame\n",
    "    target_column (str): The name of the target column\n",
    "    drop_columns (list): List of column names to drop from features. If None, use all columns except target and 'season'\n",
    "    test_size (float): Proportion of data to use for test set\n",
    "    val_size (float): Proportion of non-test data to use for validation set\n",
    "    random_state (int): Random state for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort the DataFrame by year to ensure chronological order\n",
    "    df = df.sort_values('year')\n",
    "    \n",
    "    # Define columns to drop\n",
    "    if drop_columns is None:\n",
    "        drop_columns = []\n",
    "    drop_columns = set(drop_columns + [target_column, 'year'])\n",
    "    \n",
    "    # Select feature columns (all columns except those in drop_columns)\n",
    "    feature_columns = [col for col in df.columns if col not in drop_columns]\n",
    "    \n",
    "    # Split features (X) and target (y)\n",
    "    X = df[feature_columns]\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Get unique years\n",
    "    years = df['year'].unique()\n",
    "    \n",
    "    # Calculate the number of years for test and validation\n",
    "    n_years = len(years)\n",
    "    n_test_years = max(1, int(n_years * test_size))\n",
    "    n_val_years = max(1, int((n_years - n_test_years) * val_size))\n",
    "    \n",
    "    # Split years into train, validation, and test\n",
    "    test_years = years[-n_test_years:]\n",
    "    val_years = years[-(n_test_years + n_val_years):-n_test_years]\n",
    "    train_years = years[:-(n_test_years + n_val_years)]\n",
    "    \n",
    "    # Create masks for each split\n",
    "    test_mask = df['year'].isin(test_years)\n",
    "    val_mask = df['year'].isin(val_years)\n",
    "    train_mask = df['year'].isin(train_years)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "    df_all_years,\n",
    "    target_column='win',\n",
    "    drop_columns=[\n",
    "        'season_type',\n",
    "        'team_id',\n",
    "        'opponent_id',\n",
    "        'team_conference',\n",
    "        'opponent_conference',\n",
    "        'start_date'\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.  Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'logistic_regression__C': 0.01, 'logistic_regression__penalty': 'l2', 'logistic_regression__solver': 'lbfgs', 'poly__degree': 1}\n",
      "\n",
      "Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68      1789\n",
      "           1       0.68      0.66      0.67      1789\n",
      "\n",
      "    accuracy                           0.67      3578\n",
      "   macro avg       0.67      0.67      0.67      3578\n",
      "weighted avg       0.67      0.67      0.67      3578\n",
      "\n",
      "Training Accuracy: 0.6849\n",
      "Validation Accuracy: 0.6736\n",
      "Test Accuracy: 0.6482\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "Feature 'is_home': 0.5039\n",
      "Feature 'points_allowed_last_3': 0.2823\n",
      "Feature 'points_scored_last_3': 0.2387\n",
      "Feature 'conference_game': 0.2068\n",
      "Feature 'rushingAttempts_last_3': 0.1565\n",
      "Feature 'win_rate_last_10': 0.1515\n",
      "Feature 'yardsPerRushAttempt_last_3': 0.1037\n",
      "Feature 'win_rate_last_3': 0.1030\n",
      "Feature 'rushingYards_last_3': 0.1027\n",
      "Feature 'offense_line_yards_last_10': 0.1004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_improved_logistic_regression_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Create a pipeline with more steps and options\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        ('logistic_regression', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'poly__degree': [1],\n",
    "        'logistic_regression__C': [0.01, 0.1, 1, 10],\n",
    "        'logistic_regression__penalty': ['l2'],\n",
    "        'logistic_regression__solver': ['lbfgs']\n",
    "    }\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = best_model.predict(X_train)\n",
    "    val_predictions = best_model.predict(X_val)\n",
    "    test_predictions = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importances = abs(best_model.named_steps['logistic_regression'].coef_[0])\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Print top 10 feature importances\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    for name, importance in sorted(zip(feature_names, feature_importances), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"Feature '{name}': {importance:.4f}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Usage\n",
    "improved_model = create_improved_logistic_regression_model(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.58      0.63      1789\n",
      "           1       0.64      0.74      0.69      1789\n",
      "\n",
      "    accuracy                           0.66      3578\n",
      "   macro avg       0.67      0.66      0.66      3578\n",
      "weighted avg       0.67      0.66      0.66      3578\n",
      "\n",
      "Training Accuracy: 0.6896\n",
      "Validation Accuracy: 0.6615\n",
      "Test Accuracy: 0.7195\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "Feature 'is_home': 0.1085\n",
      "Feature 'win_rate_last_10': 0.0790\n",
      "Feature 'win_rate_last_5': 0.0669\n",
      "Feature 'points_allowed_last_3': 0.0554\n",
      "Feature 'win_rate_last_3': 0.0517\n",
      "Feature 'points_scored_last_3': 0.0444\n",
      "Feature 'win_rate_last_1': 0.0332\n",
      "Feature 'offense_success_rate_last_10': 0.0245\n",
      "Feature 'offense_standard_downs_success_rate_last_10': 0.0188\n",
      "Feature 'points_allowed_last_1': 0.0187\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def create_lightweight_random_forest(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Create a pipeline with reduced complexity\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Handle null values\n",
    "        ('scaler', StandardScaler()),  # Scale features\n",
    "        ('rf', RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42))  # Reduced parameters\n",
    "    ])\n",
    "    \n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = pipeline.predict(X_train)\n",
    "    val_predictions = pipeline.predict(X_val)\n",
    "    test_predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Feature importance (top 10)\n",
    "    feature_importance = pipeline.named_steps['rf'].feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    for name, importance in sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"Feature '{name}': {importance:.4f}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Usage\n",
    "lightweight_model = create_lightweight_random_forest(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colingaffney/repos/personal-projects/college_football_predictor/venv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [10:40:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.64      0.67      1789\n",
      "           1       0.67      0.71      0.69      1789\n",
      "\n",
      "    accuracy                           0.68      3578\n",
      "   macro avg       0.68      0.68      0.68      3578\n",
      "weighted avg       0.68      0.68      0.68      3578\n",
      "\n",
      "Training Accuracy: 0.7243\n",
      "Validation Accuracy: 0.6761\n",
      "Test Accuracy: 0.7089\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "Feature 'win_rate_last_5': 0.0767\n",
      "Feature 'win_rate_last_10': 0.0613\n",
      "Feature 'is_home': 0.0441\n",
      "Feature 'win_rate_last_1': 0.0339\n",
      "Feature 'points_allowed_last_3': 0.0275\n",
      "Feature 'points_scored_last_3': 0.0207\n",
      "Feature 'offense_success_rate_last_10': 0.0179\n",
      "Feature 'conference_game': 0.0165\n",
      "Feature 'defense_total_ppa_last_10': 0.0149\n",
      "Feature 'points_scored_last_1': 0.0143\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def create_xgboost_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Handle null values\n",
    "        ('scaler', StandardScaler()),  # Scale features\n",
    "        ('xgb', XGBClassifier(\n",
    "            n_estimators=100,  # Number of boosting rounds\n",
    "            max_depth=3,       # Maximum tree depth\n",
    "            learning_rate=0.1, # Learning rate\n",
    "            subsample=0.8,     # Subsample ratio of the training instances\n",
    "            colsample_bytree=0.8, # Subsample ratio of columns when constructing each tree\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,  # Avoid warning about label encoder\n",
    "            eval_metric='logloss'     # Evaluation metric\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = pipeline.predict(X_train)\n",
    "    val_predictions = pipeline.predict(X_val)\n",
    "    test_predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Feature importance (top 10)\n",
    "    feature_importance = pipeline.named_steps['xgb'].feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    for name, importance in sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"Feature '{name}': {importance:.4f}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Usage\n",
    "xgboost_model = create_xgboost_model(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "college_football_predictor",
   "language": "python",
   "name": "college_football_predictor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
